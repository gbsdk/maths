{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb3aef11",
   "metadata": {},
   "source": [
    "# Artificial Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdb381d",
   "metadata": {},
   "source": [
    "(a type of deep learning)\n",
    "\n",
    "In a human brain, neurons communicate by sending signals to each other through complex connections known as networks.\n",
    "\n",
    "An ANN is based on the same principle to simulate the learning process of a human brain by using complex algorithms.\n",
    "\n",
    "The most common ANN structure consists of 3 components: <br>\n",
    "(1) An input layer; <br>\n",
    "(2) One or more hidden layers; and <br>\n",
    "(3) An output layer.\n",
    "\n",
    "Building an artificial neural network system is simply setting up these 3 layers in 3 steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3ed48b",
   "metadata": {},
   "source": [
    "## Step 1: Define Input Layers (Neuron = Input + Weight)\n",
    "\n",
    "An input layer is simply our regression, consisting of **inputs**/signals/independent variables {$x_{1,t}, x_{2,t}$, ...} and their corresponding **weights**/parameters/coefficients {$\\beta_1, \\beta_2$, ...}.\n",
    "\n",
    "\n",
    "Together, they form a **neuron/regression**:\n",
    "$\\beta_1x_{1,t} + \\beta_2x_{2,t} + ... $\n",
    "\n",
    "Positive weights activate the neuron (= increase the dependent variable’s values) while negative weights inhibit it (= decrease the dependent variable’s values). The net signal is sent to the hidden layer for processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29f7d21",
   "metadata": {},
   "source": [
    "## Step 2: Set up Hidden Layers (Transfer function + Bias)\n",
    "\n",
    "A hidden layer consists of a **transfer function** squashing the unbounded net signal between -$\\infty$ and $\\infty$ into a bounded output inside (0, 1) or (–1, 1).\n",
    "\n",
    "A transfer function $f(x)$ acts as a **filter** such that when the net signal reaches a critical point, it will be transferred to a different state of outcome. \n",
    "\n",
    "In practice, we usually use a **sigmoid** (S-shaped) function as the transfer function. A common sigmoid function is the **logistic function**.\n",
    "\n",
    "In neural networks, $\\beta_0$ is not called intercept but **bias**.\n",
    "\n",
    "Its role is the same as in a regression to shift the neurons so that they centre around the mean signals.\n",
    "\n",
    "If we forecast a variable taking positive and negative values (e.g. returns), the logistic function is not appropriate (why? - because the bound is (0,1)). An alternative sigmoid function squashes signals inside (–1, 1) is **hyperbolic tangent (tanh)**.\n",
    "\n",
    "Once a transfer function has been selected (say using tanh), we need to determine the number of squashers and the number of layers in our network.\n",
    "\n",
    "The more transfer functions and layers we add to the network, the more accurate our forecasts will be. However, like ARIMA modelling, adding too many squashers and layers can result in **over-fitting**, leading to bad out-of-sample forecasts.\n",
    "\n",
    "Unlike ARIMA modelling, there are no ‘rules’ to determine the number of squashers and layers. Typically, they are selected by experiment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c6ed81",
   "metadata": {},
   "source": [
    "## Step 3: Mapping Processed Signals into Output Layer\n",
    "\n",
    "An output layer consists of a target/dependent variable yt.\n",
    "\n",
    "Recall that the squashers squeeze the unbounded signals ($–\\infty, \\infty$) into bounded processed signals inside (–1, 1). The architecture/specification of sending the processed signals into the output layer depends on if we want our outputs/forecasts to be bounded or unbounded.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cc6b59",
   "metadata": {},
   "source": [
    "## Methods to Improve the forecast performance of an ANN\n",
    "\n",
    "The two ANNs in this example is far too simple. We can substantially improve the forecast performance by:\n",
    "- Adding more **neurons** (= adding variables like bond yields, USD swap rates)\n",
    "- Increasing the network’s memory (= including more lags)\n",
    "- **Training the network harder** (= **adding more hidden layers**)\n",
    "\n",
    "While a complex ANN structure can theoretically make it an extremely powerful forecasting machine, empirically it is subject to the following two limitations:\n",
    "- Finding a good set of starting values become increasingly difficult when the complexity increases.\n",
    "- In a noisy environment like the capital market, a complex ANN model can over-react (= **over-fit**), resulting in poor financial decisions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b5b399",
   "metadata": {},
   "source": [
    "https://realpython.com/python-ai-neural-network/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "936bab26",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vector = [1.72, 1.23]\n",
    "weights_1 = [1.26, 0]\n",
    "weights_2 = [2.17, 0.32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c6c76ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dot product is:2.1672\n"
     ]
    }
   ],
   "source": [
    "# Computing the dot product of input_vector and weights_1\n",
    "first_indexes_mult = input_vector[0]*weights_1[0]\n",
    "second_indexes_mult = input_vector[1]*weights_1[1]\n",
    "dot_product_0 = first_indexes_mult + second_indexes_mult\n",
    "print(f\"The dot product is:{dot_product_0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95807b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c19e6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dot product is:2.1672\n"
     ]
    }
   ],
   "source": [
    "dot_product_1 = np.dot(input_vector, weights_1)\n",
    "print(f\"The dot product is:{dot_product_1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf9c14ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dot product is:4.1259999999999994\n"
     ]
    }
   ],
   "source": [
    "dot_product_2 = np.dot(input_vector, weights_2)\n",
    "print(f\"The dot product is:{dot_product_2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f05dd037",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vector = np.array([1.66, 1.56])\n",
    "weights_1 = np.array([1.45, -0.66])\n",
    "bias = np.array([0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e82dc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/ (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba721834",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(input_vector, weights, bias):\n",
    "    layer_1 = np.dot(input_vector, weights) + bias\n",
    "    layer_2 = sigmoid(layer_1)\n",
    "    return layer_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e8847c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = make_prediction(input_vector, weights_1, bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0bcf09c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction result is: [0.87101915]\n"
     ]
    }
   ],
   "source": [
    "print(f\"The prediction result is: {prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "edaea2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vector = np.array([2, 1.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e8d5b0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = make_prediction(input_vector, weights_1, bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "55554b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction result is: [0.87101915]\n"
     ]
    }
   ],
   "source": [
    "print(f\"The prediction result is: {prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef6b565",
   "metadata": {},
   "source": [
    "# Train the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9a1e7210",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 0\n",
    "mse = np.square(prediction - target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3cc55227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:[0.87101915]; Error:[0.75867436]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Prediction:{prediction}; Error:{mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e70198",
   "metadata": {},
   "source": [
    "Remember that the error expression is error = np.square(prediction - target). When you treat (prediction - target) as a single variable x, the derivative of the error is 2 * x. By taking the derivative of this function, you want to know in what direction should you change x to bring the result of error to zero, thereby reducing the error.\n",
    "\n",
    "When it comes to your neural network, the derivative will tell you the direction you should take to update the weights variable. If it’s a positive number, then you predicted too high, and you need to decrease the weights. If it’s a negative number, then you predicted too low, and you need to increase the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d4c93335",
   "metadata": {},
   "outputs": [],
   "source": [
    "derivative = 2 * (prediction - target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0b8b28a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The derivative is [1.7420383]\n"
     ]
    }
   ],
   "source": [
    "print(f\"The derivative is {derivative}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "70689568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating the weights\n",
    "weights_1 = weights_1 - derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f99d0a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = make_prediction(input_vector, weights_1, bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "11e659fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "error = (prediction - target)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d926151f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: [0.01496248]; Error: [0.00022388]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Prediction: {prediction}; Error: {error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9babfc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
